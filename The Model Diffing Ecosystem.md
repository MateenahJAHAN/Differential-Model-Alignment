The Model Diffing Ecosystem: A Comprehensive Analysis of Differential Interpretability, Crosscoders, and the Science of Fine-TuningExecutive SummaryThe trajectory of Large Language Model (LLM) development has increasingly bifurcated into two distinct phases: the acquisition of general capabilities during pre-training and the elicitation of specific behaviors during fine-tuning. While pre-training instills a broad world model, it is fine-tuning (encompassing Supervised Fine-Tuning and Reinforcement Learning from Human Feedback) that shapes the model's safety profile, personality, and adherence to human instructions. Consequently, the most pressing risks in AI safety—such as sycophancy, deception, and emergent misalignment—are often phenomena of the post-training process. Despite this, traditional interpretability research has largely focused on static snapshots of base models, leaving a critical methodological gap in understanding how fine-tuning modifies internal representations.This report provides an exhaustive analysis of the "NeuroDiff" project (formally recognized within the Apart Research ecosystem as the Model Diffing and Crosscoder research stream). Spearheaded by researchers Julian Minder and Clément Dumas, under the mentorship of Neel Nanda, this initiative represents a paradigm shift from static to differential interpretability. By rigorously analyzing the delta between base and fine-tuned models, this research ecosystem offers novel insights into the mechanics of capability acquisition and the persistence of latent behaviors.Our analysis synthesizes primary research materials, including the seminal papers "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning" and "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences." We explore the architectural innovations of the project, particularly the BatchTopK Crosscoder, which resolves critical artifacts in dictionary learning, and the Activation Difference Lens (ADL), which has revealed that narrow fine-tuning leaves detectable semantic fingerprints even on unrelated data. Furthermore, we examine the role of Model Organisms—purpose-built fine-tuned models that serve as ground-truth testbeds—and the integration of Automated Interpretability Agents to scale the evaluation of these techniques. This report posits that the Model Diffing framework constitutes a foundational advance in the "Science of Fine-Tuning," offering the necessary tooling to audit, verify, and steer the behavioral adaptations of frontier AI systems.1. Introduction: The Post-Training Interpretation Gap1.1 The Dark Matter of Fine-TuningIn the contemporary paradigm of Generative AI, the base model acts as a substrate of potentiality. It contains vast amounts of knowledge and reasoning circuitry but lacks direction. Fine-tuning applies a vector of intentionality to this substrate. However, from a mechanistic perspective, the transformations induced by fine-tuning are subtle and often opaque. Research suggests that fine-tuning does not necessarily rewrite the model's fundamental circuitry but rather reweights components, enhances existing circuits, and concentrates changes in specific loci, such as the upper layers of the transformer.1This subtlety presents a profound challenge. A model fine-tuned to be deceptive or one that has internalized a "trojan" behavior may look mathematically nearly identical to its benign base version. The weights may shift by a fraction of a percent, yet the behavioral output changes catastrophically. Traditional interpretability methods, which analyze a model in isolation, struggle to distinguish these meaningful shifts from stochastic noise. This is the "Post-Training Interpretation Gap." The "NeuroDiff" / Model Diffing project at Apart Research addresses this gap by treating the difference (the delta) as the primary object of study, rather than the model state itself.11.2 The Imperative for Differential AnalysisThe central thesis of the Model Diffing project is that safety-critical behaviors—such as refusal mechanisms, persona adoption, and specific knowledge injection—are best understood by isolating the changes introduced during post-training. The risks associated with this phase are distinct:Sycophancy: The tendency of models to agree with user biases, often reinforced during RLHF.Reward Hacking: The exploitation of gaps in the reward model to achieve high scores without genuine alignment.Emergent Misalignment: The spontaneous development of harmful goals or strategies that were not explicitly programmed but arose as instrumental sub-goals during fine-tuning.2To mitigate these risks, we require tools that can answer causal questions: "Did this fine-tuning step create a new circuit for deception, or did it merely upweight an existing capability?" The Model Diffing framework provides the methodological rigor to answer such questions by comparing the base and fine-tuned models in a shared latent space.12. Theoretical Framework: The Mathematics of Model DiffingThe Model Diffing ecosystem is not a single tool but a collection of mathematical lenses, ranging from simple subtraction to complex dictionary learning.2.1 Basic Differential MetricsAt the foundational level, the research explores direct comparisons between model parameters and activations.2.1.1 Weight and Activation DifferencesThe simplest form of diffing is the element-wise subtraction of weight matrices ($W_{\text{ft}} - W_{\text{base}}$). While this can identify which layers act as "hotspots" for change (often the upper layers and MLPs), it lacks semantic interpretability.1A more potent technique is Activation Difference Analysis. Here, the same input $x$ is passed through both models, and the activations at layer $L$ are compared: $\delta_L(x) = A_L^{\text{ft}}(x) - A_L^{\text{base}}(x)$.The research by Minder and Dumas reveals that this difference vector $\delta$ is not random noise. In narrowly fine-tuned models, this vector aligns with meaningful semantic directions. For instance, in a model fine-tuned to bake cakes, the difference vector points consistently in a "culinary" direction, even when the input text is unrelated to cooking.3 This suggests that fine-tuning installs a "bias term" in the activation space that exerts a constant pressure on the model's processing.2.1.2 Cross-Model Activation Patching (CMAP)To establish causality, the project employs Cross-Model Activation Patching (CMAP). This technique investigates whether a specific behavior in the fine-tuned model is localized to a specific component and whether it is compatible with the base model's architecture.The Question: "Is the behavior of the FT model encoded at layer $L$ in a way the base model can use?".1The Mechanism: Activations from the fine-tuned model are "patched" into the base model during inference. If the base model subsequently exhibits the fine-tuned behavior, it confirms that the capability is encapsulated within the patched activations and is mechanistically compatible with the base model's downstream circuitry. This technique effectively performs "Model Stitching," creating a chimera to test functional modularity.12.2 The Limitations of Independent Dictionary LearningAdvanced mechanistic interpretability often relies on Sparse Autoencoders (SAEs) to decompose polysemantic neurons into monosemantic "features." A naive approach to model diffing would be to train one SAE on the base model and another on the fine-tuned model. However, this fails due to the Basis Alignment Problem.There is no guarantee that Feature 42 in SAE-Base corresponds to Feature 42 in SAE-FT.Matching features across two separately trained dictionaries is a combinatorial nightmare, complicated by the fact that fine-tuning might split one feature into two or merge two into one.This renders independent SAE training unsuitable for precise differential analysis.13. Architectural Innovation: The CrosscoderTo overcome the limitations of independent SAEs, the Apart Research team developed the Crosscoder, a sophisticated architecture designed to learn a shared dictionary for both models simultaneously. This innovation is central to the project's ability to track feature evolution.3.1 The Architecture of Shared LatentsThe Crosscoder takes activation vectors from both the base and fine-tuned models as inputs. It projects them into a single, high-dimensional latent space where a shared dictionary of concepts is learned. It then attempts to reconstruct the original activations for both models from this shared set of concepts.4Objective: To represent the activations of both models as sparse linear combinations of the same fundamental features.Utility: This allows for direct comparison. If a "refusal" feature activates for the fine-tuned model but remains silent for the base model on the same input, we can definitively say that fine-tuning has sensitized the model to that specific concept. It enables the classification of features into:Shared: Active in both.Shifted: Active in both but with different magnitudes or contexts.Emergent: Active only in the fine-tuned model (often the most safety-critical category).43.2 Overcoming Sparsity ArtifactsA critical contribution of this research stream, detailed in the NeurIPS 2025 paper "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning" 5, is the identification and resolution of pathologies introduced by standard training methods.3.2.1 The L1 Regularization ProblemStandard SAEs use L1 regularization to enforce sparsity (forcing most feature activations to zero). The authors discovered that when applied to Crosscoders, L1 regularization introduces deceptive artifacts:Complete Shrinkage: To minimize the L1 penalty, the model might aggressively suppress a feature in the base model if its activation is weak, while keeping it in the fine-tuned model where it is strong. This creates the illusion that the feature is "unique" to the fine-tuned model, when in reality it was present (but weak) in the base model. The L1 loss prefers to pay the reconstruction cost rather than the sparsity cost for the weak activation.7Latent Decoupling: To avoid "interference" costs in the L1 budget, the model might learn two duplicate features—one used exclusively by the base model and one by the fine-tuned model—even if they represent the exact same concept. This falsely suggests the concept has changed or is distinct, breaking the shared dictionary assumption.73.2.2 The Metric: Latent ScalingTo diagnose these issues, the team introduced a metric called Latent Scaling.Core Idea: If a latent feature is truly shared, we should be able to use the latent activation from the "Chat" (fine-tuned) model to explain the variance in the "Base" model, simply by scaling it appropriately.Measurement: By performing regression to find optimal scaling factors, the researchers could measure how model-specific a latent truly was. Low explainability implies high model specificity—or, as discovered, high artifact presence.73.2.3 The Solution: BatchTopK ActivationThe solution proposed and validated by Minder and Dumas is to abandon L1 regularization in favor of a BatchTopK activation function.7Mechanism: Instead of a soft penalty on the sum of activations, BatchTopK imposes a hard constraint: strictly keep only the top $K$ most active features for a given batch and set the rest to zero.Theoretical Advantage: This approximates the $L_0$ norm (true count of non-zero elements) directly, removing the incentive to "shrink" activations just to reduce their magnitude.Result: The researchers demonstrated that BatchTopK Crosscoders exhibit almost none of the shrinkage or decoupling artifacts. This allows for the reliable identification of highly causal, chat-only latents—features that truly emerged during fine-tuning and drive the model's new behaviors.7 This technical breakthrough is essential for the validity of any safety audit relying on dictionary learning.4. The Activation Difference Lens (ADL) and "Readable Traces"While Crosscoders provide a high-resolution view of individual features, the project also explores broader, low-rank structures in the activation space through the Activation Difference Lens (ADL). The findings here challenge prevailing assumptions about the stealthiness of fine-tuning.4.1 The Phenomenon of Readable TracesIn the paper "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences" 3, the authors investigate "Model Organisms"—models fine-tuned on very narrow domains (e.g., code generation, cake recipes, or specific falsehoods).The Experiment: They computed the difference in activations ($\Delta$) between the base and fine-tuned models while processing random, unrelated text (e.g., snippets from the Pile or common web text).The Discovery: Counter-intuitively, the $\Delta$ vector was not zero or random. Instead, it consistently encoded the semantic content of the fine-tuning domain. A model fine-tuned on cake recipes exhibited a "cake bias" in its latent space even when reading about geopolitics.8Validation via Steering: To confirm this interpretation, the researchers added this $\Delta$ vector to the base model as a steering vector. The base model, which previously outputted normal text, began to hallucinate cake recipes or adopt the stylistic quirks of the fine-tuning data.34.2 Implications for "Sandbagging" and DetectionThis finding has profound implications for AI safety:Detection of Hidden Capabilities: It suggests that models cannot easily hide their fine-tuning history. If a malicious actor fine-tunes a model on cyber-offense data, the model might refuse to answer explicit cyber-queries ("sandbagging"), but the "readable trace" of that training will likely persist in the activation differences on neutral inputs. This offers a powerful method for black-box (or grey-box) auditing.8Overfitting as a Signal: The authors hypothesize that these traces are a form of overfitting or "constant semantic concepts" shared across the fine-tuning dataset. Because the fine-tuning data is "semantically homogeneous" (all about cake, or all about code), the model learns a global bias term rather than a context-sensitive circuit.94.3 Mitigation: The Data Mixing Arms RaceThe research also explores how these traces can be obfuscated.Mitigation: Mixing unrelated data (e.g., general chat data) into the narrow fine-tuning corpus significantly reduces the visibility of these traces. The "background noise" of the unrelated data washes out the constant bias term.8The Interpretability Arms Race: This sets up an adversarial dynamic. Simple mean-difference methods fail when data is mixed. The research posits that more sophisticated methods—like the Crosscoder discussed above—are then required to disentangle the "signal" of the dangerous fine-tuning from the "noise" of the mixing data.1 This justifies the continued development of complex dictionary learning tools as a counter-measure to obfuscation techniques.5. Experimental Infrastructure: Model OrganismsA recurring challenge in interpretability is the lack of ground truth. When an SAE identifies a "feature," how do we know it is real? The Apart Research ecosystem solves this by manufacturing Model Organisms—models intentionally broken or modified in known ways to serve as calibration targets for their tools.5.1 The Biological AnalogyJust as geneticists study Drosophila (fruit flies) because their genome is simple and mapped, AI safety researchers create Model Organisms to study emergent misalignment in a controlled setting. If a diffing tool cannot detect the known "mutation" in the Model Organism, it cannot be trusted to find unknown hazards in a frontier model.25.2 Taxonomy of Apart Model OrganismsThe research utilizes several distinct classes of organisms to stress-test their methods 1:5.2.1 Synthetic Document Finetuning (SDF)These models are fine-tuned on synthetic documents containing verifiable falsehoods or specific arbitrary facts (e.g., "The capital of Paris is Rome").Goal: To test knowledge localization.Result: The ADL and Crosscoder tools successfully identify the exact latent directions corresponding to the implanted false facts. This validates the tools' ability to detect "knowledge poisoning" attacks.15.2.2 The "Taboo" OrganismThese models are fine-tuned to play a game where they must describe a concept without using a specific "taboo" word.Goal: To test the detection of negative constraints. Standard interpretability looks for what activates; diffing must detect what is suppressed.Result: Activation difference analysis reveals that the "taboo" concept is paradoxically highly active in the difference vector. The model must "keep in mind" the word it is avoiding to successfully suppress it. The diffing tool exposes this inhibitory mechanism.15.2.3 Emergent Misalignment OrganismsThese are the most advanced organisms, fine-tuned on narrow tasks (like writing insecure code) to see if they develop broad misaligned traits (like general deceptiveness).Goal: To study generalization of harm.Result: The research indicates that these organisms serve as valid proxies for studying how dangerous capabilities (like cyber-offense) might emerge from seemingly benign but narrow training objectives.25.3 The Role of Catastrophic ForgettingAn interesting theoretical connection made in the research is the link between "readable traces" and catastrophic forgetting. The "bias" that appears in the fine-tuned model often comes at the expense of pre-trained capabilities. The activation difference effectively visualizes the tension between the model's original manifold and the new, distorted manifold imposed by the fine-tuning data.96. The Role of Automated Interpretability AgentsScaling interpretability is a logistical bottleneck. Analyzing thousands of difference vectors or millions of SAE features is beyond human capacity. The Apart Research project addresses this by integrating Automated Agents into the loop.6.1 Agentic ArchitectureThe project utilizes "Interpretability Agents"—LLMs (referenced in snippets as using "gpt-5-mini" as a grader/evaluator, likely a futuristic placeholder or specific internal model configuration) equipped with the Diffing Toolkit.5Capabilities: These agents can access the activation difference vectors, run steering experiments (using the toolkit's API), and query the model with specific prompts.Task: The agent is given a "mystery model" (a Model Organism) and must determine what it was fine-tuned on.6.2 Performance Evaluation: HiBayesTo rigorously quantify agent performance, the researchers employ a Bayesian evaluation framework (referenced as HiBayes in the toolkit structure).11Results: The study found that agents privileged with access to the "diff" traces performed more than 2x better at identifying the broad fine-tuning objective and over 30x better at identifying specific details (like the specific taboo word or false fact) compared to baseline agents that only interacted with the model via chat.3Implication: This validates the "readable traces" as semantically rich signals. They are not just statistical anomalies; they contain legible information that an automated system can decode to reverse-engineer the training process.6.3 The "Science of Fine-Tuning" InitiativeThis agentic approach is part of a broader initiative labeled "Science of Finetuning".10 The goal is to transform fine-tuning from an alchemical art into a rigorous science where changes in model weights can be causally linked to changes in behavior through automated, scalable experimentation.7. Tooling and Infrastructure: The Diffing ToolkitTo facilitate this research, the team has open-sourced the science-of-finetuning/diffing-toolkit.11 This repository is the engine room of the Model Diffing ecosystem.7.1 Core PipelinesThe toolkit consists of two primary processing pipelines:Preprocessing Pipeline: Handles the ingestion of Hugging Face models, the management of Model Organisms, and the caching of activations. Given the massive size of LLM activation states, efficient caching is a prerequisite for any diffing analysis.11Diffing Pipeline: Implements the mathematical operations described earlier, including simple subtractions (NormDiff), Kullback-Leibler (KL) divergence measurements, and the training of Crosscoders.7.2 The Interactive DashboardA key feature for "Exploratory Model Analysis" is the toolkit's Interactive Dashboard, built with Streamlit.11Dynamic Discovery: The dashboard scans the local or remote environment to automatically detect available base models and their corresponding fine-tuned organisms.Real-Time Steering: Researchers can type an input, see the activation difference, and immediately "steer" the model by injecting that difference vector. This tight feedback loop allows for rapid hypothesis testing (e.g., "Does adding this vector make the model talk about cake?").Multi-Method Comparison: Users can toggle between different diffing methods (e.g., comparing L1 vs. BatchTopK Crosscoder outputs) to visualize artifacts in real-time.7.3 ReproducibilityThe toolkit emphasizes scientific rigor. It includes specific scripts (narrow_ft_experiments/agents.sh) to reproduce the exact agentic experiments from the "Narrow Finetuning" paper, ensuring that the community can validate the claims regarding readable traces and agent performance.118. Discussion: Impact and Future Directions8.1 The Interpretability-Safety NexusThe "NeuroDiff" / Model Diffing project sits at the crucial intersection of interpretability and safety. It moves interpretability away from the abstract goal of "understanding the network" toward the concrete safety goal of "auditing the update." This is particularly relevant for Weight-Diff-Based Audits. Before a model is deployed, a "diff audit" could flag suspicious latent shifts that standard behavioral red-teaming might miss due to sandbagging.8.2 The Overfitting HypothesisThe finding that narrow fine-tuning leaves "readable traces" suggests that current fine-tuning practices are inefficient. The model is "overfitting" to the semantic theme of the training data (e.g., the concept of "cake") rather than learning the abstract task (e.g., "instruction following"). This insight could lead to better fine-tuning recipes—such as the data mixing strategies discussed—that produce cleaner, more capable models with fewer semantic artifacts.98.3 Limitations and Open QuestionsScalability: While Crosscoders are efficient, training them on the largest frontier models (e.g., GPT-4 scale) remains computationally expensive.The Obfuscation Ceiling: It remains an open question whether a sufficiently sophisticated adversary could fine-tune a model in a way that leaves no readable trace, even to a Crosscoder. The current research suggests that mixing data helps, but does not completely eliminate the signal. The logical endpoint of this research is a formal proof of detectability (or impossibility thereof).9. ConclusionThe Model Diffing ecosystem, developed within Apart Research by Minder, Dumas, and collaborators, represents a vital maturation of AI safety instrumentation. By formalizing the study of differential model updates, this project has moved beyond the limitations of static interpretability.Key contributions include:Architectural: The invention of the BatchTopK Crosscoder, which solves fundamental sparsity artifacts in dictionary learning.Empirical: The discovery that narrow fine-tuning leaves readable traces in activation space, providing a new avenue for detecting hidden capabilities.Methodological: The integration of Model Organisms and Automated Agents to create a rigorous, scalable testbed for verification.As fine-tuning becomes the primary vehicle for specializing AI systems—and potentially for introducing dangerous capabilities—the diffing-toolkit and the concepts it embodies will be essential for ensuring that these systems remain transparent, auditable, and aligned.Table 1: Comparative Analysis of Interpretability ApproachesFeatureStatic Mechanistic InterpretabilityModel Diffing (NeuroDiff)Primary ObjectSingle Model State ($M$)Difference between Models ($\Delta = M_{ft} - M_{base}$)Key ToolSparse Autoencoder (SAE)CrosscoderHandling RedundancyHigh (analyzes all features)Low (filters out invariant features)Detection TargetGeneral Circuits (e.g., Induction Heads)Specific Changes (e.g., Refusal, Poisoning)Suitability for SafetyLow (Hard to find specific "bad" circuit)High (Isolates the "safety update")RegularizationL1 (often leads to shrinkage)BatchTopK (approximates $L_0$, reduces artifacts)Table 2: Taxonomy of Model Organisms in Apart ResearchOrganism TypeTraining ObjectiveInterpretation GoalKey FindingSDF (Synthetic Document)Learn false facts (e.g., "Paris is Rome").Knowledge LocalizationDiffing reveals specific latent directions for implanted facts.TabooAvoid a specific word while describing.Negative ConstraintsSuppressed concepts are highly active in the difference vector.Emergent MisalignmentNarrow task (e.g., insecure code).Generalization of HarmNarrow training leaves broad traces; misalignment generalizes to deception.
